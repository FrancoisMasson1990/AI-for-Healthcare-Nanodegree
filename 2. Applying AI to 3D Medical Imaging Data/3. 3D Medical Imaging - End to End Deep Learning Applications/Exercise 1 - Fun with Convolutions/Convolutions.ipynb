{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practicing convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook you will practice applying 2D, 2.5D and 3D convolutions to a medical volume using PyTorch. Conveniently, PyTorch offers the functionality of computing convolutions with arbitrary kernel sizes, and handles all the mechanics of striding and padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydicom\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy.ma as ma\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import nibabel as nib\n",
    "from PIL import Image\n",
    "import math\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (7,7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying convolutional filters\n",
    "\n",
    "In this section we will give you some starter code on how to apply 2D convolution using PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a 4x4 edge filter kernel\n",
    "\n",
    "conv_kernel = np.ones((4,4))\n",
    "conv_kernel[2:,:] = -1\n",
    "print(conv_kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use Torch's convolutional layer for our convolutional filter operation\n",
    "\n",
    "conv2d = nn.Conv2d(\n",
    "    1, # Input size == 1 (we are dealing with 1 input channel)\n",
    "    1, # Output size - we want to get 1 channel as an output\n",
    "    kernel_size = (4, 4), # size of our filter kernel\n",
    "    bias = False) # We do not need a bias for this operation\n",
    "conv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's turn our convolutional kernel into a tensor which we can use to initialize our convolutional layer\n",
    "\n",
    "params = torch.from_numpy(conv_kernel).type(torch.FloatTensor).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "# Note the unsqeeze operation - this is effectively adding empty dimensions to the vector bringing it to 4 dimensions\n",
    "# Torch expects parameter vector of size (output_channels, input_channels, kernel x dimension, kernel y dimension)\n",
    "\n",
    "conv2d.weight = torch.nn.Parameter(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load an image of the adorable walrus:\n",
    "\n",
    "walrus = Image.open('data/walrus.jpg')\n",
    "plt.imshow(walrus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's convert it to grayscale and normalize (because this is what our convolution operation is expecting)\n",
    "\n",
    "walrus = walrus.convert(\"L\")\n",
    "walrus = np.array(walrus)\n",
    "walrus = walrus.astype(np.single)/0xff\n",
    "\n",
    "plt.imshow(walrus, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conv2D layer is expecting a 4D tensor (batch_size, channels, width, height)\n",
    "# Let's bring the walrus into proper shape. We have batch of size one and only one channel\n",
    "# so we will use the unsqueeze operation for this\n",
    "\n",
    "walrus_tensor = torch.from_numpy(walrus).unsqueeze(0).unsqueeze(1)\n",
    "walrus_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Let's compute the convolution map and ReLU activations. Note that we also used the time \"magic function\"\n",
    "# to see how long it takes. Later on, you will compare other convolution methods that you will try.\n",
    "\n",
    "convolved = conv2d(walrus_tensor)\n",
    "relu = F.relu(convolved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# And let's visualize them!\n",
    "\n",
    "plt.imshow(np.squeeze(convolved.detach().numpy()), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.squeeze(relu.detach().numpy()), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hurray, we've run a 2D convolutional layer with custom kernel, using PyTorch. Onwards to medical applications!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a NIFTI volume\n",
    "\n",
    "Remember how to use NiBabel to load those NIFTI volumes? Here's a refresher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load our image\n",
    "\n",
    "nii_img = nib.load(\"data/spleen.nii.gz\")\n",
    "img = nii_img.get_fdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize a slice:\n",
    "\n",
    "plt.imshow(img[:,:,0], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By now you should've seen enough of these to recognize that you are looking at an abdominal cross-section. Now you are ready to apply convolutions!\n",
    "\n",
    "But before we go there - a couple of notes about pixel sizes, on this particular image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize sagittal cross-section at cut 250 (we will use rot90 since we want to orient the image it properly)\n",
    "\n",
    "plt.imshow(np.rot90(img[250,:,:]), cmap = \"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, that looks squished!\n",
    "\n",
    "Remember our conversations about anisotropic voxels ? As you might have guessed, our pixels are much shorter in z dimension than they are in x and y. Let's see what we can do about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nii_img.header[\"pixdim\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember how we scaled DICOM image in previous lesson, to account for difference between in-plane resolution and resolution across z-axis? In NIFTI one extracts proper voxel aspect ratio from NIFTI image using the pixdim field.\n",
    "\n",
    "Unlike DICOM, NIFTI files store all their pixel dimensions in a single place - pixdim field. Also unlike DICOM this field also stores a bunch of other stuff related to uses of NIFTI format beyond static 3D images. If you're curious what those are - check out the [NIFTI documentation](https://nifti.nimh.nih.gov/nifti-1/documentation/nifti1fields/nifti1fields_pages/pixdim.html/document_view). \n",
    "\n",
    "For the purpose of this exercise we are interested in values at locations 1, 2 and 3 - these are our x, y and z dimensions respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img2d = np.rot90(img[250,:,:])\n",
    "plt.imshow(img2d, cmap = \"gray\", aspect=nii_img.header[\"pixdim\"][3]/nii_img.header[\"pixdim\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will also give you a convenience function to visualize all slices in a grid as thumbnails:\n",
    "\n",
    "def display_volume_slices(img, w, h):\n",
    "    plot_w = w\n",
    "    plot_h = h\n",
    "\n",
    "    # You can play with figsize parameter to adjust how large the images are\n",
    "    fig, ax = plt.subplots(plot_h, plot_w, figsize=[35,35])\n",
    "\n",
    "    for i in range(plot_w*plot_h):\n",
    "        plt_x = i % plot_w\n",
    "        plt_y = i // plot_w\n",
    "        if (i < len(img)):\n",
    "            ax[plt_y, plt_x].set_title(f\"slice {i}\")\n",
    "            ax[plt_y, plt_x].imshow(img[i], cmap='gray')\n",
    "        ax[plt_y, plt_x].axis(\"off\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# And let's visualize ALL slices\n",
    "#\n",
    "# Note that we are calling np.transpose here because our display_volume_slices iterates over 0th dimension\n",
    "# of the input volume. Our Nibabel volumes have z dimension stored in the 2nd position, so we \n",
    "# move the z-dimension in front of the other two here\n",
    "\n",
    "display_volume_slices(np.transpose(img, (2, 0, 1)), 7, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2D Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK**: Using the kernel we have defined above (or any other kernel you like), use the \"2D Convolution\" method to create a volume of convolution maps for all of the slices in our volume. Visualize them.\n",
    "\n",
    "Use %%time (as we've done above) to see how much time it takes.  \n",
    "How many parameters does define the convolution operation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===============SOLUTION================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Let's just reuse the convolutional layer defined above and apply it to every slice to get our volume\n",
    "\n",
    "conv_slices = []\n",
    "\n",
    "for ix in range(0, img.shape[2]):\n",
    "    slice_tensor = torch.from_numpy((img[:,:,ix].astype(np.single)/0xff)).unsqueeze(0).unsqueeze(1)\n",
    "    convolved = conv2d(slice_tensor)\n",
    "    conv_slices.append(np.squeeze(convolved.detach().numpy()))\n",
    "\n",
    "plt.imshow(conv_slices[20], cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's visualize the slices. See how our filter makes areas of the image where there's a lot going on stand out?\n",
    "\n",
    "display_volume_slices(conv_slices, 7, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at our parameter vector - how many knobs does our convolution operation have in total?\n",
    "# You can guess for this one - that's the size of our conv matrix\n",
    "\n",
    "conv2d.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of trainable parameters: {np.prod(conv2d.weight.shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# 3D Convolutions\n",
    "\n",
    "**TASK**: Using a 3D version of the same kernel, compute and time full 3D convolutions by using PyTorch's Conv3D layer. Note that our kernel represents a 2D edge filter. What would be a 3D edge filter?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "===============SOLUTION================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv3d = nn.Conv3d(\n",
    "    1, # Input size == 1 (we are dealing with 1 input channel)\n",
    "    1, # Output size - we want to get 1 channel as an output\n",
    "    kernel_size = (4, 4, 4), # size of our kernel filter\n",
    "    bias = False) # We do not need a bias for this operation\n",
    "conv3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the kernel\n",
    "conv_kernel3d = np.array([conv_kernel, conv_kernel, -conv_kernel, -conv_kernel])\n",
    "conv_kernel3d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm what form of the weights is expected\n",
    "conv3d.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up weights\n",
    "params3d = torch.from_numpy(conv_kernel3d).type(torch.FloatTensor).unsqueeze(0).unsqueeze(0) \n",
    "conv3d.weight = torch.nn.Parameter(params3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert image to tensor that works for our conv3d filter\n",
    "conv_tensor = torch.from_numpy((img.astype(np.single)/0xff)).unsqueeze(0).unsqueeze(1)\n",
    "conv_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Run convolution\n",
    "convolved = conv3d(conv_tensor)\n",
    "convolved.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "convolved_np = np.transpose(np.squeeze(convolved.detach().numpy()), (2, 0, 1))\n",
    "convolved_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "display_volume_slices(convolved_np, 7, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of trainable parameters: {np.prod(conv3d.weight.shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# 2.5D Convolutions\n",
    "\n",
    "> **Extra credit exercise**: _Depending on how familiar you are with Numpy and PyTorch, this exercise may take up to a few hours to complete. This exercise may help you understand the architecture of convolutions better, but is not essential to understanding the course. If you feel like this would be too time consuming you are welcome to move on or take a peek at the solution._\n",
    "\n",
    "Now Let's try 2.5D convolutions. That's a bit more difficult since we want to be specific about how exactly we select data for our extra planes, and also for it to be meaningful we would like to combine data in small areas around area of interest which means we would need to process image in patches. This is not something PyTorch offers right out of the box, and you would engage in something like this if you are conscious of performance, so you would want to have control over this anyway. A bit more coding will be required than previous exercises. \n",
    "\n",
    "We will try to recreate the approach to building convolutions which has been presented in [this paper from the NIH](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4295635/). Specifically, we will build a layer that runs a convolutional filter over patches that look like this:\n",
    "\n",
    "<img src=\"convolutions.img/25d.jpg\" width=\"200\">\n",
    "\n",
    "**TASK**: Using same kernel, compute and time creation of a volume of convolutional feature maps using the 2.5D convolutions approach by looking at 16x16x16 patches and extracting three square segments from the center of the patch, along the cardinal planes, similar to how it was described in the lesson video. Treat the three 16x16 segments as input channels for your convolutional layer still using one output channel). Visualize all axial slices of your convolutional feature map.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convenience function - will extract patch_size square patches from 3D img across cardinal planes with center in the \n",
    "# middle of each patch and put them into (num_patches, 3, patch_size, patch_size) numpy array\n",
    "\n",
    "# It assumes that img.shape is divisible by patch_size for the sake of simplicity (which is reasonable - you would \n",
    "# likely want to normalize your data anyway)\n",
    "\n",
    "def extract_patches(img, patch_size, z_level):\n",
    "    \n",
    "    # create holder array\n",
    "    num_patches = (img.shape[0]//patch_size) * (img.shape[1]//patch_size)\n",
    "    out = np.zeros((num_patches, 3, patch_size, patch_size))\n",
    "    \n",
    "    for p_x in range(0,img.shape[0]//patch_size):\n",
    "        for p_y in range(0,img.shape[1]//patch_size):\n",
    "            \n",
    "            # Figure out where the patch should start in our main plane\n",
    "            patch_start_x = p_x*patch_size\n",
    "            patch_end_x = patch_start_x + patch_size\n",
    "            x_center = patch_start_x + patch_size//2\n",
    "\n",
    "            patch_start_y = p_y*patch_size\n",
    "            patch_end_y = patch_start_y + patch_size\n",
    "            y_center = patch_start_y + patch_size//2\n",
    "\n",
    "            # Figure out where patch starts in ortho planes. \n",
    "            # Note that we extract patches in orthogonal direction, therefore indices might go over\n",
    "            # or go negative\n",
    "            patch_start_z = max(0, z_level-patch_size//2)\n",
    "            patch_end_z = patch_start_z + patch_size\n",
    "\n",
    "            if (patch_end_z >= img.shape[2]):\n",
    "                patch_end_z -= patch_end_z - img.shape[2]\n",
    "                patch_start_z = patch_end_z - patch_size\n",
    "\n",
    "            # Get axial, sagittal and coronal slices, assuming particular arrangement of respective planes in the \n",
    "            # input image\n",
    "            patch_a = img[patch_start_x:patch_end_x, patch_start_y:patch_end_y, z_level]\n",
    "            patch_s = img[x_center, patch_start_y:patch_end_y, patch_start_z:patch_end_z]\n",
    "            patch_c = img[patch_start_x:patch_end_x, y_center, patch_start_z:patch_end_z]\n",
    "\n",
    "            patch_id = p_x*img.shape[0]//patch_size + p_y\n",
    "            out[patch_id] = np.array([patch_a, patch_s, patch_c])\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another convenience function to rebuild image from patches, assuming square patches contribute to square image\n",
    "def build_slice_from_patches(patches):\n",
    "    img_size = int(math.sqrt(patches.shape[0]))*patches.shape[1]\n",
    "\n",
    "    out = np.zeros((img_size, img_size))\n",
    "    \n",
    "    for i in range(patches.shape[0]):\n",
    "        x = i // (out.shape[0] // patches.shape[2]) \n",
    "        y = i % (out.shape[0] // patches.shape[2]) \n",
    "\n",
    "        x_start = x*patches.shape[2]\n",
    "        x_end = x_start + patches.shape[2]\n",
    "\n",
    "        y_start = y*patches.shape[2]\n",
    "        y_end = y_start + patches.shape[2]\n",
    "\n",
    "        out[x_start:x_end, y_start:y_end] = patches[i]\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the two - extract some patches, and put it back together and visualize\n",
    "\n",
    "patches = extract_patches(img, patch_size, 40)\n",
    "patches.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rebuild and visualize axials\n",
    "plt.imshow(build_slice_from_patches(np.squeeze(patches[:,0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking good, now other planes?\n",
    "plt.imshow(build_slice_from_patches(np.squeeze(patches[:,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking trippy! Anatomy seems distorted but that's something to be expected - remember that our volume has pixels that are non-square? Probably in real world if you wanted to use this in a CNN you would want to rescale everything to square voxels, with some sort of interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's set up the convolutional operator\n",
    "\n",
    "conv25d = nn.Conv2d(\n",
    "    3, # Input size == 1 (we are dealing with 3 input channels - one per each orthogonal plane in a patch)\n",
    "    1, # Output size - we want to get 1 channel as an output\n",
    "    kernel_size = (4, 4), # size of our filter kernel\n",
    "    bias = False) # We do not need a bias for this operation\n",
    "conv25d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm the shape of weights\n",
    "\n",
    "conv25d.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up conv kernel (we will recycle the one from the prev lesson). \n",
    "# Remember that we are using 3 conv kernels - one per channel\n",
    "conv_kernel25d = np.array([conv_kernel, conv_kernel, conv_kernel])\n",
    "\n",
    "# Initialize layer parameters with the kernel\n",
    "# Torch expects parameter vector of size (output_channels, input_channels, kernel x dimension, kernel y dimension)\n",
    "params = torch.from_numpy(conv_kernel25d).type(torch.FloatTensor).unsqueeze(0)\n",
    "conv25d.weight = torch.nn.Parameter(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Now, run our convolution operation on all the layers:\n",
    "\n",
    "conv_slices = []\n",
    "for z in range(img.shape[2]):\n",
    "    slice_tensor = torch.from_numpy((extract_patches(img, patch_size, z).astype(np.single)/0xff))\n",
    "    convolved = conv25d(slice_tensor)\n",
    "    conv_slices.append(np.squeeze(convolved.detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a slice\n",
    "plt.imshow(build_slice_from_patches(conv_slices[20]), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a bunch of slices\n",
    "\n",
    "display_volume_slices([build_slice_from_patches(p) for p in conv_slices], 7, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"Number of trainable parameters: {np.prod(conv25d.weight.shape)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
